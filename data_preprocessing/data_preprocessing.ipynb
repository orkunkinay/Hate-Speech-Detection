{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('../dataset/train.csv')\n",
    "test_df = pd.read_csv('../dataset/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/orkunkinay/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/orkunkinay/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/orkunkinay/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  label                                              tweet  \\\n",
      "0   1      0   @user when a father is dysfunctional and is s...   \n",
      "1   2      0  @user @user thanks for #lyft credit i can't us...   \n",
      "2   3      0                                bihday your majesty   \n",
      "3   4      0  #model   i love u take with u all the time in ...   \n",
      "4   5      0             factsguide: society now    #motivation   \n",
      "\n",
      "                                       cleaned_tweet  \n",
      "0  father dysfunctional selfish drag kid dysfunct...  \n",
      "1  thanks lyft credit cant use cause dont offer w...  \n",
      "2                                     bihday majesty  \n",
      "3                        model love u take u time ur  \n",
      "4                      factsguide society motivation  \n",
      "      id                                              tweet  \\\n",
      "0  31963  #studiolife #aislife #requires #passion #dedic...   \n",
      "1  31964   @user #white #supremacists want everyone to s...   \n",
      "2  31965  safe ways to heal your #acne!!    #altwaystohe...   \n",
      "3  31966  is the hp and the cursed child book up for res...   \n",
      "4  31967    3rd #bihday to my amazing, hilarious #nephew...   \n",
      "\n",
      "                                       cleaned_tweet  \n",
      "0  studiolife aislife requires passion dedication...  \n",
      "1  white supremacist want everyone see new bird m...  \n",
      "2   safe way heal acne altwaystoheal healthy healing  \n",
      "3  hp cursed child book reservation already yes h...  \n",
      "4  rd bihday amazing hilarious nephew eli ahmir u...  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\@\\w+|\\#','', text)\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "train_df['cleaned_tweet'] = train_df['tweet'].apply(preprocess_text)\n",
    "test_df['cleaned_tweet'] = test_df['tweet'].apply(preprocess_text)\n",
    "\n",
    "original_train_df = train_df.copy()\n",
    "\n",
    "print(train_df.head())\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations after removing outliers: 31957\n"
     ]
    }
   ],
   "source": [
    "# Method 1: Remove outliers based on text length\n",
    "Q1 = train_df['cleaned_tweet'].apply(len).quantile(0.25)\n",
    "Q3 = train_df['cleaned_tweet'].apply(len).quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "outlier_threshold_high = Q3 + 1.5 * IQR\n",
    "\n",
    "train_df_removed_outliers = train_df[train_df['cleaned_tweet'].apply(len) <= outlier_threshold_high]\n",
    "print(f\"Number of observations after removing outliers: {train_df_removed_outliers.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations after capping outliers: 31962\n"
     ]
    }
   ],
   "source": [
    "# Method 2: Cap outliers at threshold\n",
    "outlier_threshold_high = int(outlier_threshold_high)\n",
    "\n",
    "train_df_capped_outliers = train_df.copy()\n",
    "train_df_capped_outliers['cleaned_tweet'] = train_df_capped_outliers['cleaned_tweet'].apply(\n",
    "    lambda x: x if len(x) <= outlier_threshold_high else x[:outlier_threshold_high]\n",
    ")\n",
    "print(f\"Number of observations after capping outliers: {train_df_capped_outliers.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def get_bert_embeddings(texts, batch_size=32):\n",
    "    embeddings = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        encodings = tokenizer(batch_texts, truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**encodings)\n",
    "            batch_embeddings = outputs.last_hidden_state[:, 0, :].numpy()\n",
    "            embeddings.append(batch_embeddings)\n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "train_features_removed_outliers = get_bert_embeddings(train_df_removed_outliers['cleaned_tweet'].tolist())\n",
    "train_features_capped_outliers = get_bert_embeddings(train_df_capped_outliers['cleaned_tweet'].tolist())\n",
    "test_features = get_bert_embeddings(test_df['cleaned_tweet'].tolist())\n",
    "\n",
    "np.save('train_features_removed_outliers.npy', train_features_removed_outliers)\n",
    "np.save('train_features_capped_outliers.npy', train_features_capped_outliers)\n",
    "np.save('test_features.npy', test_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution after manual oversampling (removed outliers):\n",
      "0    29715\n",
      "1    29715\n",
      "dtype: int64\n",
      "Class distribution after manual oversampling (capped outliers):\n",
      "0    29720\n",
      "1    29720\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def manual_oversample(X, y):\n",
    "    X_majority = X[y == 0]\n",
    "    y_majority = y[y == 0]\n",
    "    X_minority = X[y == 1]\n",
    "    y_minority = y[y == 1]\n",
    "\n",
    "    num_samples_to_generate = len(y_majority) - len(y_minority)\n",
    "\n",
    "    indices = np.random.choice(range(len(X_minority)), size=num_samples_to_generate, replace=True)\n",
    "    X_oversampled = np.vstack([X_majority, X_minority, X_minority[indices]])\n",
    "    y_oversampled = np.hstack([y_majority, y_minority, y_minority[indices]])\n",
    "\n",
    "    return X_oversampled, y_oversampled\n",
    "\n",
    "X_resampled_removed, y_resampled_removed = manual_oversample(train_features_removed_outliers, train_df_removed_outliers['label'].values)\n",
    "X_resampled_capped, y_resampled_capped = manual_oversample(train_features_capped_outliers, train_df_capped_outliers['label'].values)\n",
    "\n",
    "print(\"Class distribution after manual oversampling (removed outliers):\")\n",
    "print(pd.Series(y_resampled_removed).value_counts())\n",
    "print(\"Class distribution after manual oversampling (capped outliers):\")\n",
    "print(pd.Series(y_resampled_capped).value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution after custom SMOTE (removed outliers):\n",
      "0    29715\n",
      "1    29715\n",
      "dtype: int64\n",
      "Class distribution after custom SMOTE (capped outliers):\n",
      "0    29720\n",
      "1    29720\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def custom_smote(X, y, k_neighbors=5):\n",
    "    X_majority = X[y == 0]\n",
    "    y_majority = y[y == 0]\n",
    "    X_minority = X[y == 1]\n",
    "    y_minority = y[y == 1]\n",
    "\n",
    "    neighbors = NearestNeighbors(n_neighbors=k_neighbors).fit(X_minority)\n",
    "    num_samples_to_generate = len(y_majority) - len(y_minority)\n",
    "    X_synthetic = []\n",
    "    y_synthetic = []\n",
    "\n",
    "    for _ in range(num_samples_to_generate):\n",
    "        index = np.random.randint(0, len(X_minority))\n",
    "        nn_index = neighbors.kneighbors(X_minority[index].reshape(1, -1), return_distance=False).flatten()\n",
    "        neighbor = X_minority[nn_index[np.random.randint(1, k_neighbors)]]\n",
    "        diff = neighbor - X_minority[index]\n",
    "        gap = np.random.rand()\n",
    "        X_synthetic.append(X_minority[index] + gap * diff)\n",
    "        y_synthetic.append(1)\n",
    "\n",
    "    X_smote = np.vstack([X_majority, X_minority, np.array(X_synthetic)])\n",
    "    y_smote = np.hstack([y_majority, y_minority, np.array(y_synthetic)])\n",
    "\n",
    "    return X_smote, y_smote\n",
    "\n",
    "X_smote_removed, y_smote_removed = custom_smote(train_features_removed_outliers, train_df_removed_outliers['label'].values)\n",
    "X_smote_capped, y_smote_capped = custom_smote(train_features_capped_outliers, train_df_capped_outliers['label'].values)\n",
    "\n",
    "print(\"Class distribution after custom SMOTE (removed outliers):\")\n",
    "print(pd.Series(y_smote_removed).value_counts())\n",
    "print(\"Class distribution after custom SMOTE (capped outliers):\")\n",
    "print(pd.Series(y_smote_capped).value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy with Base Data: 95.06%\n",
      "Validation Accuracy after Oversampling (removed outliers): 99.48%\n",
      "Validation Accuracy after Oversampling (capped outliers): 99.78%\n",
      "Validation Accuracy after SMOTE (removed outliers): 99.30%\n",
      "Validation Accuracy after SMOTE (capped outliers): 99.22%\n"
     ]
    }
   ],
   "source": [
    "class HateSpeechDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feature = self.features[idx]\n",
    "        label = self.labels[idx]\n",
    "        return torch.tensor(feature, dtype=torch.float), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "class HateSpeechClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(HateSpeechClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "def train_model(train_loader, model, criterion, optimizer, epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for features, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "    return model\n",
    "\n",
    "def evaluate_model(val_loader, model):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for features, labels in val_loader:\n",
    "            outputs = model(features)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "hidden_dim = 128\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_features_removed_outliers, train_df_removed_outliers['label'].values, test_size=0.2, random_state=42)\n",
    "val_dataset = HateSpeechDataset(X_val, y_val)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train_dataset_base = HateSpeechDataset(X_train, y_train)\n",
    "train_loader_base = DataLoader(train_dataset_base, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "train_dataset_ros_removed = HateSpeechDataset(X_resampled_removed, y_resampled_removed)\n",
    "train_loader_ros_removed = DataLoader(train_dataset_ros_removed, batch_size=batch_size, shuffle=True)\n",
    "train_dataset_ros_capped = HateSpeechDataset(X_resampled_capped, y_resampled_capped)\n",
    "train_loader_ros_capped = DataLoader(train_dataset_ros_capped, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "train_dataset_smote_removed = HateSpeechDataset(X_smote_removed, y_smote_removed)\n",
    "train_loader_smote_removed = DataLoader(train_dataset_smote_removed, batch_size=batch_size, shuffle=True)\n",
    "train_dataset_smote_capped = HateSpeechDataset(X_smote_capped, y_smote_capped)\n",
    "train_loader_smote_capped = DataLoader(train_dataset_smote_capped, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model_base = HateSpeechClassifier(input_dim=X_train.shape[1], hidden_dim=hidden_dim, output_dim=2)\n",
    "model_ros_removed = HateSpeechClassifier(input_dim=X_resampled_removed.shape[1], hidden_dim=hidden_dim, output_dim=2)\n",
    "model_ros_capped = HateSpeechClassifier(input_dim=X_resampled_capped.shape[1], hidden_dim=hidden_dim, output_dim=2)\n",
    "model_smote_removed = HateSpeechClassifier(input_dim=X_smote_removed.shape[1], hidden_dim=hidden_dim, output_dim=2)\n",
    "model_smote_capped = HateSpeechClassifier(input_dim=X_smote_capped.shape[1], hidden_dim=hidden_dim, output_dim=2)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_base = optim.Adam(model_base.parameters(), lr=learning_rate)\n",
    "optimizer_ros_removed = optim.Adam(model_ros_removed.parameters(), lr=learning_rate)\n",
    "optimizer_ros_capped = optim.Adam(model_ros_capped.parameters(), lr=learning_rate)\n",
    "optimizer_smote_removed = optim.Adam(model_smote_removed.parameters(), lr=learning_rate)\n",
    "optimizer_smote_capped = optim.Adam(model_smote_capped.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "model_base = train_model(train_loader_base, model_base, criterion, optimizer_base, epochs=10)\n",
    "accuracy_base = evaluate_model(val_loader, model_base)\n",
    "print(f'Validation Accuracy with Base Data: {accuracy_base * 100:.2f}%')\n",
    "\n",
    "model_ros_removed = train_model(train_loader_ros_removed, model_ros_removed, criterion, optimizer_ros_removed, epochs=10)\n",
    "accuracy_ros_removed = evaluate_model(val_loader, model_ros_removed)\n",
    "print(f'Validation Accuracy after Oversampling (removed outliers): {accuracy_ros_removed * 100:.2f}%')\n",
    "\n",
    "model_ros_capped = train_model(train_loader_ros_capped, model_ros_capped, criterion, optimizer_ros_capped, epochs=10)\n",
    "accuracy_ros_capped = evaluate_model(val_loader, model_ros_capped)\n",
    "print(f'Validation Accuracy after Oversampling (capped outliers): {accuracy_ros_capped * 100:.2f}%')\n",
    "\n",
    "model_smote_removed = train_model(train_loader_smote_removed, model_smote_removed, criterion, optimizer_smote_removed, epochs=10)\n",
    "accuracy_smote_removed = evaluate_model(val_loader, model_smote_removed)\n",
    "print(f'Validation Accuracy after SMOTE (removed outliers): {accuracy_smote_removed * 100:.2f}%')\n",
    "\n",
    "model_smote_capped = train_model(train_loader_smote_capped, model_smote_capped, criterion, optimizer_smote_capped, epochs=10)\n",
    "accuracy_smote_capped = evaluate_model(val_loader, model_smote_capped)\n",
    "print(f'Validation Accuracy after SMOTE (capped outliers): {accuracy_smote_capped * 100:.2f}%')\n",
    "\n",
    "torch.save(model_ros_removed.state_dict(), 'hate_speech_classifier_ros_removed.pth')\n",
    "torch.save(model_ros_capped.state_dict(), 'hate_speech_classifier_ros_capped.pth')\n",
    "torch.save(model_smote_removed.state_dict(), 'hate_speech_classifier_smote_removed.pth')\n",
    "torch.save(model_smote_capped.state_dict(), 'hate_speech_classifier_smote_capped.pth')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
